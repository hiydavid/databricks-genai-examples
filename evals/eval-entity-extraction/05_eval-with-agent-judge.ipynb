{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "5bff59f4",
            "metadata": {},
            "source": [
                "# 05. Evaluate with Agent-as-Judge\n",
                "\n",
                "This notebook demonstrates how to use the `make_judge` API with the `{{trace}}` variable to create an \"agent-as-judge\". This allows the judge to evaluate the full execution trace of the agent, not just the inputs and outputs.\n",
                "\n",
                "This is somewhat new and still WIP."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42032d28",
            "metadata": {},
            "outputs": [],
            "source": [
                "%run ./00_setup.ipynb"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f3b3e63a",
            "metadata": {},
            "source": [
                "## Load evaluation data as records"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bbfbb2c0",
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_dataset = mlflow.genai.datasets.get_dataset(\n",
                "    name=f\"{CATALOG}.{SCHEMA}.{EVAL_TABLE}\",\n",
                ")\n",
                "\n",
                "eval_records = eval_dataset.to_df()[[\"inputs\", \"expectations\"]].to_dict(\n",
                "    orient=\"records\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "987e1f94",
            "metadata": {},
            "source": [
                "## Define Agent-as-Judge\n",
                "\n",
                "We define a custom judge that has access to the `{{trace}}` variable. This variable contains the JSON representation of the agent's execution trace."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bb772e0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "from mlflow.genai.judges import make_judge\n",
                "\n",
                "JUDGE_MODEL = \"databricks:/databricks-claude-sonnet-4\"\n",
                "\n",
                "agent_trace_judge = make_judge(\n",
                "    name=\"agent_trace_quality\",\n",
                "    instructions=\"\"\"Evaluate the quality of {{ trace }} for the following:\n",
                "    1. Was the response format used to extract entities from the raw text input?\n",
                "    2. Did the extract include all the entities (\n",
                "        \"start_date\", \"end_date\", \"leased_space\", \"lessee\", \"lessor\", \"signing_date\", \n",
                "        \"term_of_payment\", \"designated_use\", \"extension_period\", \"expiration_date_of_lease\")?\n",
                "    Your response must be a boolean: yes (if the trace looks good) or no.\"\"\",\n",
                "    model=JUDGE_MODEL,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ynrt5uz7tpg",
            "metadata": {},
            "source": [
                "## Run evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e858d7fd",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pprint import pprint\n",
                "from IPython.display import Markdown, display"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a09f985e",
            "metadata": {},
            "outputs": [],
            "source": [
                "result = extract_lease_data(eval_records[0][\"inputs\"][\"query\"])\n",
                "\n",
                "sample_trace_id = mlflow.get_last_active_trace_id()\n",
                "sample_trace = mlflow.get_trace(sample_trace_id)\n",
                "\n",
                "feedback = agent_trace_judge(trace=sample_trace)\n",
                "\n",
                "display(Markdown(f\"**Performance Rating:** {feedback.value}\"))\n",
                "display(Markdown(f\"**Analysis:** {feedback.rationale}\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
