{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Optimization\n",
    "\n",
    "In this notebook, we will use `mlflow.genai.optimize_prompts` to optimize our lease extraction prompt. After optimization, we will evaluate the performance using the held-out evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04902a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.optimize import GepaPromptOptimizer\n",
    "import pandas as pd\n",
    "from utils import (\n",
    "    setup_mlflow,\n",
    "    load_config,\n",
    "    create_predict_fn,\n",
    "    create_fuzzy_scorers,\n",
    "    EXTRACTION_FIELDS,\n",
    ")\n",
    "\n",
    "setup_mlflow()\n",
    "config = load_config()\n",
    "\n",
    "print(f\"User: {config['user']}\")\n",
    "print(f\"Experiment: {config['mlflow_experiment_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9780cd8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full training data from Delta table\n",
    "train_table_name = f\"{config['catalog']}.{config['schema']}.prompt_opt_training\"\n",
    "train_sdf = spark.read.table(train_table_name)\n",
    "print(f\"Total training records: {train_sdf.count()}\")\n",
    "\n",
    "# Load eval dataset from MLflow\n",
    "eval_dataset_name = f\"{config['catalog']}.{config['schema']}.prompt_opt_eval\"\n",
    "eval_dataset = mlflow.genai.datasets.get_dataset(eval_dataset_name)\n",
    "eval_records = eval_dataset.to_df()[[\"inputs\", \"expectations\"]].to_dict(\n",
    "    orient=\"records\"\n",
    ")\n",
    "print(f\"Total eval records: {len(eval_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab4fc6",
   "metadata": {},
   "source": [
    "## Optimizing prompt with GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data\n",
    "train_pdf = train_sdf.toPandas()\n",
    "\n",
    "# Convert to records format\n",
    "train_records = []\n",
    "for _, row in train_pdf.iterrows():\n",
    "    record = {\n",
    "        \"inputs\": {\"query\": row.get(\"request\", \"\")},\n",
    "        \"expectations\": {\"expected_response\": row.get(\"labels\", \"\")},\n",
    "    }\n",
    "    train_records.append(record)\n",
    "\n",
    "print(f\"Records for optimization: {len(train_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6faa0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt_uri = (\n",
    "    f\"prompts:/{config['catalog']}.{config['schema']}.lease_extraction_prompt/1\"\n",
    ")\n",
    "\n",
    "predict_fn_base = create_predict_fn(base_prompt_uri)\n",
    "\n",
    "# Create fuzzy scorers for optimization\n",
    "fuzzy_scorers = create_fuzzy_scorers(threshold=0.7)\n",
    "\n",
    "\n",
    "# Aggregation function to average all fuzzy match scores\n",
    "def aggregation_fn(scores: dict) -> float:\n",
    "    \"\"\"Average all fuzzy match scores across fields.\"\"\"\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for field in EXTRACTION_FIELDS:\n",
    "        feedback = scores.get(field)\n",
    "        if feedback is not None:\n",
    "            total += float(feedback.value)\n",
    "            count += 1\n",
    "    return total / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"Optimization\"):\n",
    "    optimization = mlflow.genai.optimize_prompts(\n",
    "        predict_fn=predict_fn_base,\n",
    "        train_data=train_records,\n",
    "        prompt_uris=[base_prompt_uri],\n",
    "        optimizer=GepaPromptOptimizer(\n",
    "            reflection_model=f\"databricks:/{config['optimizer_endpoint']}\",\n",
    "            max_metric_calls=len(train_records) * 3,\n",
    "        ),\n",
    "        scorers=fuzzy_scorers,\n",
    "        aggregation=aggregation_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d82edd",
   "metadata": {},
   "source": [
    "## Evaluate optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt_uri = (\n",
    "    f\"prompts:/{config['catalog']}.{config['schema']}.lease_extraction_prompt/4\"\n",
    ")\n",
    "\n",
    "predict_fn_new = create_predict_fn(optimized_prompt_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh scorers for eval\n",
    "eval_scorers = create_fuzzy_scorers(threshold=0.7)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Eval Round\"):\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=eval_records,\n",
    "        predict_fn=predict_fn_new,\n",
    "        scorers=eval_scorers,\n",
    "    )\n",
    "\n",
    "# Print per-field scores and overall average\n",
    "mean_scores = []\n",
    "for key, value in eval_results.metrics.items():\n",
    "    if \"/mean\" in key:\n",
    "        print(f\"{key}: {value:.2%}\")\n",
    "        mean_scores.append(value)\n",
    "\n",
    "if mean_scores:\n",
    "    print(f\"\\nOverall average: {sum(mean_scores) / len(mean_scores):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6f976",
   "metadata": {},
   "source": [
    "With 70 training samples, the GEPA-optimized prompt was able to achieve an increase in average accuracy by more than **16%** across the extraction fields."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
