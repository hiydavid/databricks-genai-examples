{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Optimization\n",
    "\n",
    "In this notebook, we will use `mlflow.genai.optimize_prompts` to optimize our lease extraction prompt. After optimization, we will evaluate the performance using the held-out evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04902a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import Correctness\n",
    "from mlflow.genai.optimize import GepaPromptOptimizer\n",
    "import pandas as pd\n",
    "from utils import setup_mlflow, load_config, create_predict_fn\n",
    "\n",
    "setup_mlflow()\n",
    "config = load_config()\n",
    "\n",
    "print(f\"User: {config['user']}\")\n",
    "print(f\"Experiment: {config['mlflow_experiment_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9780cd8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full training data from Delta table\n",
    "train_table_name = f\"{config['catalog']}.{config['schema']}.prompt_opt_training\"\n",
    "train_sdf = spark.read.table(train_table_name)\n",
    "print(f\"Total training records: {train_sdf.count()}\")\n",
    "\n",
    "# Load eval dataset from MLflow\n",
    "eval_dataset_name = f\"{config['catalog']}.{config['schema']}.prompt_opt_eval\"\n",
    "eval_dataset = mlflow.genai.datasets.get_dataset(eval_dataset_name)\n",
    "eval_records = eval_dataset.to_df()[[\"inputs\", \"expectations\"]].to_dict(\n",
    "    orient=\"records\"\n",
    ")\n",
    "print(f\"Total eval records: {len(eval_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab4fc6",
   "metadata": {},
   "source": [
    "## Optimizing prompt with GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data\n",
    "train_pdf = train_sdf.toPandas()\n",
    "\n",
    "# Convert to records format\n",
    "train_records = []\n",
    "for _, row in train_pdf.iterrows():\n",
    "    record = {\n",
    "        \"inputs\": {\"query\": row.get(\"request\", \"\")},\n",
    "        \"expectations\": {\"expected_response\": row.get(\"labels\", \"\")},\n",
    "    }\n",
    "    train_records.append(record)\n",
    "\n",
    "print(f\"Records for optimization: {len(train_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6faa0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt_uri = (\n",
    "    f\"prompts:/{config['catalog']}.{config['schema']}.lease_extraction_prompt/1\"\n",
    ")\n",
    "\n",
    "predict_fn_base = create_predict_fn(base_prompt_uri)\n",
    "\n",
    "\n",
    "# Aggregation function to convert Correctness feedback to numerical score\n",
    "def aggregation_fn(scores: dict) -> float:\n",
    "    \"\"\"Convert Correctness Feedback to numerical score (1.0 for pass, 0.0 for fail).\"\"\"\n",
    "    correctness_feedback = scores.get(\"correctness\")\n",
    "    if correctness_feedback is None:\n",
    "        return 0.0\n",
    "    # Feedback.value is \"yes\"/\"no\" or similar categorical value\n",
    "    value = correctness_feedback.value\n",
    "    return 1.0 if value in (\"yes\", \"PASS\", True) else 0.0\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"Optimization\"):\n",
    "    optimization_first_half = mlflow.genai.optimize_prompts(\n",
    "        predict_fn=predict_fn_base,\n",
    "        train_data=train_records,\n",
    "        prompt_uris=[base_prompt_uri],\n",
    "        optimizer=GepaPromptOptimizer(\n",
    "            reflection_model=f\"databricks:/{config['optimizer_endpoint']}\",\n",
    "            max_metric_calls=len(train_records) * 3,\n",
    "        ),\n",
    "        scorers=[Correctness()],\n",
    "        aggregation=aggregation_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d82edd",
   "metadata": {},
   "source": [
    "## Evaluate optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt_uri = (\n",
    "    f\"prompts:/{config['catalog']}.{config['schema']}.lease_extraction_prompt/3\"\n",
    ")\n",
    "\n",
    "predict_fn_new = create_predict_fn(optimized_prompt_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Eval Round\"):\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=eval_records,\n",
    "        predict_fn=predict_fn_new,\n",
    "        scorers=[Correctness()],\n",
    "    )\n",
    "\n",
    "print(f\"New Prompt's Correctness: {eval_results.metrics['correctness/mean']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6f976",
   "metadata": {},
   "source": [
    "With 70 training samples, the GEPA-optimized prompt was able to achieve an increase in correctness of nearly 10%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
