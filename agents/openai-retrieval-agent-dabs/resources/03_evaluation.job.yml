resources:
  jobs:
    agent_evaluation:
      name: "retrieval-agent-evaluation-${bundle.target}"
      description: "Evaluate agent quality using MLflow evaluation framework"
      max_concurrent_runs: 1
      queue:
        enabled: true

      tasks:
        - task_key: run_evaluation
          description: "Run agent evaluation with MLflow"
          notebook_task:
            notebook_path: ../src/03_evaluation.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              experiment_name: ${resources.experiments.retrieval_agent_experiment.name}
          timeout_seconds: 3600
