{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Agentic Workflow with Databricks Lakeflow Jobs\n",
    "\n",
    "This notebook demonstrates how to use Databricks Lakeflow Jobs to execute async agentic workflows.\n",
    "\n",
    "**Key Demo Points:**\n",
    "- Interactive agent for research planning (Planner Agent)\n",
    "- Async job execution via Lakeflow (Researcher Agent)\n",
    "- Non-blocking polling for job status\n",
    "- Results saved to Unity Catalog Volume\n",
    "\n",
    "## Flow\n",
    "1. Converse with Planner Agent to create a research plan\n",
    "2. Approve the plan → triggers async Lakeflow Job\n",
    "3. Continue working while job runs (poll for updates as needed)\n",
    "4. Retrieve completed report from UC Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-sdk databricks-mcp openai pydantic pyyaml --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport yaml\n\n# Auto-detect src path from notebook location\n# Notebook is in src/notebooks/, so src is ../\nSRC_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\nprint(f\"Auto-detected SRC_PATH: {SRC_PATH}\")\n\nsys.path.insert(0, SRC_PATH)\n\n# Load configuration from config.yaml\nconfig_path = os.path.join(SRC_PATH, \"config.yaml\")\nwith open(config_path, \"r\") as f:\n    config = yaml.safe_load(f)\n\nprint(f\"Loaded config from: {config_path}\")\n\n# Verify imports work\nfrom models.research_plan import ResearchPlan\nfrom planner_agent import PlannerAgent\nfrom job_tools import check_job_status\n\nprint(\"Imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these values for your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - Loaded from config.yaml\n",
    "# ============================================\n",
    "\n",
    "# LLM endpoint (Databricks Foundation Model)\n",
    "LLM_ENDPOINT = config[\"llm\"][\"endpoint_name\"]\n",
    "\n",
    "# Path to the researcher notebook in your workspace\n",
    "RESEARCHER_NOTEBOOK_PATH = config[\"paths\"][\"researcher_notebook\"]\n",
    "\n",
    "# UC Volume path for output reports\n",
    "OUTPUT_VOLUME_PATH = config[\"paths\"][\"output_volume\"]\n",
    "\n",
    "print(f\"LLM Endpoint: {LLM_ENDPOINT}\")\n",
    "print(f\"Researcher Notebook: {RESEARCHER_NOTEBOOK_PATH}\")\n",
    "print(f\"Output Volume: {OUTPUT_VOLUME_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Planner Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "ws = WorkspaceClient()\n",
    "print(f\"Connected to: {ws.config.host}\")\n",
    "\n",
    "agent = PlannerAgent(\n",
    "    llm_endpoint=LLM_ENDPOINT,\n",
    "    researcher_notebook_path=RESEARCHER_NOTEBOOK_PATH,\n",
    "    output_volume_path=OUTPUT_VOLUME_PATH,\n",
    "    workspace_client=ws,\n",
    ")\n",
    "\n",
    "print(\"Planner Agent initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Interactive Conversation\n\nRun the cell below to start an interactive conversation with the Planner Agent.\n\n**Conversation flow:**\n1. Describe your research topic\n2. Agent proposes research questions\n3. Refine the questions through conversation\n4. Say \"run it\" or \"execute\" when ready → Agent submits async Lakeflow Job\n5. Type \"quit\" or \"exit\" to end the conversation\n\nThe Planner Agent can:\n- Help you develop research questions\n- Submit approved plans to a Lakeflow Job for async execution\n- Check job status\n- Retrieve completed reports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Interactive conversation loop\n# Type your messages and press Enter. Type \"quit\" or \"exit\" to end.\n\nprint(\"=\" * 60)\nprint(\"PLANNER AGENT - Interactive Research Planning\")\nprint(\"=\" * 60)\nprint(\"Describe your research topic to get started.\")\nprint(\"Type 'quit' or 'exit' to end the conversation.\")\nprint(\"=\" * 60)\nprint()\n\nwhile True:\n    try:\n        user_input = input(\"You: \").strip()\n    except EOFError:\n        break\n    \n    if not user_input:\n        continue\n    \n    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n        print(\"\\nEnding conversation. Active jobs will continue running.\")\n        break\n    \n    # Send to agent and get response\n    response = agent.chat(user_input)\n    print(f\"\\nAgent: {response}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## After Exiting the Conversation\n\nOnce you exit the interactive loop, you can still:\n- Check job status with `agent.get_active_jobs()`\n- Resume conversation with `agent.chat(\"your message\")`\n- Reset conversation with `agent.reset_conversation()`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check status of all active jobs\nfor job in agent.get_active_jobs():\n    print(f\"Run ID: {job['run_id']}\")\n    print(f\"  Topic: {job['topic']}\")\n    print(f\"  State: {job['state']}\")\n    print(f\"  Output: {job['output_path']}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Results\n",
    "\n",
    "Once the job completes, retrieve the research report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the agent to retrieve the report\n",
    "# Only works after job completes\n",
    "response = agent.chat(\"Show me the research report\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Read directly from UC Volume\n",
    "# Replace with actual output path\n",
    "# output_path = \"/Volumes/{catalog}/{schema}/{volume}/report_xxx.md\"\n",
    "# with open(output_path, 'r') as f:\n",
    "#     report = f.read()\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "This demo shows:\n",
    "\n",
    "1. **Async Execution**: The Planner Agent kicks off a Lakeflow Job and returns immediately - it doesn't wait.\n",
    "\n",
    "2. **Long-Running Tasks**: The Researcher Agent can run for >5 minutes (up to the job timeout) without blocking.\n",
    "\n",
    "3. **Non-Blocking Polling**: Check job status anytime without waiting for completion.\n",
    "\n",
    "4. **UC Volume Output**: Results are persisted to Unity Catalog Volume for reliable retrieval.\n",
    "\n",
    "5. **MCP Tool Integration**: The Researcher Agent uses Databricks MCP for web search capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-job-run",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}