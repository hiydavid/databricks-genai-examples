{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Agentic Workflow with Databricks Lakeflow Jobs\n",
    "\n",
    "This notebook demonstrates how to use Databricks Lakeflow Jobs to execute async agentic workflows.\n",
    "\n",
    "**Key Demo Points:**\n",
    "- Interactive agent for research planning (Planner Agent)\n",
    "- Async job execution via Lakeflow (Researcher Agent)\n",
    "- Non-blocking polling for job status\n",
    "- Results saved to Unity Catalog Volume\n",
    "\n",
    "## Flow\n",
    "1. Converse with Planner Agent to create a research plan\n",
    "2. Approve the plan â†’ triggers async Lakeflow Job\n",
    "3. Continue working while job runs (poll for updates as needed)\n",
    "4. Retrieve completed report from UC Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-sdk databricks-mcp openai pydantic --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path for imports\n",
    "# Update this path to match your workspace location\n",
    "SRC_PATH = \"/Workspace/Users/{your_email}/agent-job-run/src\"\n",
    "sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "# Verify imports work\n",
    "from models.research_plan import ResearchPlan\n",
    "from planner_agent import PlannerAgent\n",
    "from job_tools import check_job_status\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these values for your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - UPDATE THESE VALUES\n",
    "# ============================================\n",
    "\n",
    "# LLM endpoint (Databricks Foundation Model)\n",
    "LLM_ENDPOINT = \"databricks-claude-sonnet-4\"\n",
    "\n",
    "# Path to the researcher notebook in your workspace\n",
    "RESEARCHER_NOTEBOOK_PATH = \"/Workspace/Users/{your_email}/agent-job-run/src/notebooks/02_researcher_job\"\n",
    "\n",
    "# UC Volume path for output reports\n",
    "# Format: /Volumes/{catalog}/{schema}/{volume}\n",
    "OUTPUT_VOLUME_PATH = \"/Volumes/{catalog}/{schema}/{volume}\"\n",
    "\n",
    "print(f\"LLM Endpoint: {LLM_ENDPOINT}\")\n",
    "print(f\"Researcher Notebook: {RESEARCHER_NOTEBOOK_PATH}\")\n",
    "print(f\"Output Volume: {OUTPUT_VOLUME_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Planner Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "ws = WorkspaceClient()\n",
    "print(f\"Connected to: {ws.config.host}\")\n",
    "\n",
    "agent = PlannerAgent(\n",
    "    llm_endpoint=LLM_ENDPOINT,\n",
    "    researcher_notebook_path=RESEARCHER_NOTEBOOK_PATH,\n",
    "    output_volume_path=OUTPUT_VOLUME_PATH,\n",
    "    workspace_client=ws,\n",
    ")\n",
    "\n",
    "print(\"Planner Agent initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Conversation\n",
    "\n",
    "Use the `agent.chat()` method to converse with the Planner Agent.\n",
    "\n",
    "**Example conversation flow:**\n",
    "1. \"I want to research the impact of AI on healthcare\"\n",
    "2. Agent proposes research questions\n",
    "3. \"Looks good, but add a question about regulatory challenges\"\n",
    "4. Agent updates questions\n",
    "5. \"Perfect, let's run it\"\n",
    "6. Agent submits async job and returns run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the conversation - describe your research topic\n",
    "response = agent.chat(\"I want to research trends in generative AI adoption in enterprise\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation - refine the plan\n",
    "response = agent.chat(\"Those questions look good. Let's run the research.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Job Status\n",
    "\n",
    "The research job is now running asynchronously. You can:\n",
    "- Continue with other work in this notebook\n",
    "- Poll for job status as needed\n",
    "- The main agent doesn't wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status of active jobs\n",
    "active_jobs = agent.get_active_jobs()\n",
    "for job in active_jobs:\n",
    "    print(f\"Run ID: {job['run_id']}\")\n",
    "    print(f\"  Topic: {job['topic']}\")\n",
    "    print(f\"  State: {job['state']}\")\n",
    "    print(f\"  Output: {job['output_path']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the agent about job status\n",
    "# Replace with actual run_id from the job submission\n",
    "response = agent.chat(\"What's the status of the research job?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Results\n",
    "\n",
    "Once the job completes, retrieve the research report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the agent to retrieve the report\n",
    "# Only works after job completes\n",
    "response = agent.chat(\"Show me the research report\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Read directly from UC Volume\n",
    "# Replace with actual output path\n",
    "# output_path = \"/Volumes/{catalog}/{schema}/{volume}/report_xxx.md\"\n",
    "# with open(output_path, 'r') as f:\n",
    "#     report = f.read()\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "This demo shows:\n",
    "\n",
    "1. **Async Execution**: The Planner Agent kicks off a Lakeflow Job and returns immediately - it doesn't wait.\n",
    "\n",
    "2. **Long-Running Tasks**: The Researcher Agent can run for >5 minutes (up to the job timeout) without blocking.\n",
    "\n",
    "3. **Non-Blocking Polling**: Check job status anytime without waiting for completion.\n",
    "\n",
    "4. **UC Volume Output**: Results are persisted to Unity Catalog Volume for reliable retrieval.\n",
    "\n",
    "5. **MCP Tool Integration**: The Researcher Agent uses Databricks MCP for web search capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
