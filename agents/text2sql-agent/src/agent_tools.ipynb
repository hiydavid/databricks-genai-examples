{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc5d084",
   "metadata": {},
   "source": [
    "# Create Agent Tools as UC Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b354e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "spark = DatabricksSession.builder.remote(serverless=True).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2cdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "configs = mlflow.models.ModelConfig(development_config=\"./config.yml\")\n",
    "databricks_config = configs.get(\"databricks\")\n",
    "tools_config = configs.get(\"tools\")\n",
    "\n",
    "CATALOG = databricks_config[\"catalog\"]\n",
    "SCHEMA = databricks_config[\"schema\"]\n",
    "SQL_WAREHOUSE_ID = databricks_config[\"sql_warehouse_id\"]\n",
    "WORKSPACE_URL = databricks_config[\"workspace_url\"]\n",
    "SECRET_SCOPE_NAME = databricks_config[\"databricks_pat\"][\"secret_scope_name\"]\n",
    "SECRET_KEY_NAME = databricks_config[\"databricks_pat\"][\"secret_key_name\"]\n",
    "\n",
    "UC_TABLES = tools_config[\"tables\"]\n",
    "UC_CONNECTION = tools_config[\"uc_connection\"][\"name\"]\n",
    "UC_CONNECTION_SQL_EXEC = UC_CONNECTION + \"_sql_exec\"\n",
    "\n",
    "table_list_sql = \", \".join([f\"'{table}'\" for table in UC_TABLES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6348b4",
   "metadata": {},
   "source": [
    "## Setup UC Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b90103",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE CONNECTION IF NOT EXISTS {UC_CONNECTION} \n",
    "    TYPE HTTP\n",
    "    OPTIONS (\n",
    "        host '{WORKSPACE_URL}',\n",
    "        port '443',\n",
    "        base_path '/api/2.1/',\n",
    "        bearer_token secret('{SECRET_SCOPE_NAME}', '{SECRET_KEY_NAME}'\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71503aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE CONNECTION IF NOT EXISTS {UC_CONNECTION_SQL_EXEC} \n",
    "    TYPE HTTP\n",
    "    OPTIONS (\n",
    "        host '{WORKSPACE_URL}',\n",
    "        port '443',\n",
    "        base_path '/api/2.0/',\n",
    "        bearer_token secret('{SECRET_SCOPE_NAME}', '{SECRET_KEY_NAME}'\n",
    "        )\n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d03ee9",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29244f2",
   "metadata": {},
   "source": [
    "### Get Table Metadata\n",
    "* Purpose: Returns comprehensive table-level metadata including owner, storage location, timestamps, and table type via Unity Catalog REST API\n",
    "* Why Essential: Provides high-level table information that helps the agent understand what tables are available, who owns them, and when they were last updated - critical for discovery and context before schema introspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.get_table_metadata(\n",
    "        table_name STRING COMMENT 'Fully qualified table name (catalog.schema.table)'\n",
    "    )\n",
    "    RETURNS STRUCT<\n",
    "        name: STRING,\n",
    "        catalog_name: STRING,\n",
    "        schema_name: STRING,\n",
    "        table_type: STRING,\n",
    "        data_source_format: STRING,\n",
    "        comment: STRING,\n",
    "        owner: STRING,\n",
    "        created_at: BIGINT,\n",
    "        updated_at: BIGINT,\n",
    "        storage_location: STRING,\n",
    "        full_name: STRING\n",
    "    >\n",
    "    COMMENT 'Returns enhanced table metadata via Databricks REST API including owner, storage location, and timestamps'\n",
    "    LANGUAGE SQL\n",
    "    RETURN (\n",
    "        WITH api_response AS (\n",
    "            SELECT http_request(\n",
    "                conn => '{UC_CONNECTION}',\n",
    "                method => 'GET',\n",
    "                path => concat('unity-catalog/tables/', table_name),\n",
    "                headers => map('Accept', 'application/json')\n",
    "            ) as response\n",
    "        )\n",
    "        SELECT \n",
    "            from_json(\n",
    "                response.text,\n",
    "                'name STRING, catalog_name STRING, schema_name STRING, table_type STRING, data_source_format STRING, comment STRING, owner STRING, created_at BIGINT, updated_at BIGINT, storage_location STRING, full_name STRING'\n",
    "            ) as metadata\n",
    "        FROM api_response\n",
    "        WHERE response.status_code = 200\n",
    "    );\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b84622",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT {CATALOG}.{SCHEMA}.get_table_metadata('{UC_TABLES[0]}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9674e",
   "metadata": {},
   "source": [
    "### Get Table Schema\n",
    "* Purpose: Returns detailed column-level metadata including names, data types, descriptions, nullability, and position from Unity Catalog via REST API\n",
    "* Why Essential: Core function that provides the schema blueprint the LLM needs to generate accurate SQL - includes column descriptions that are crucial for semantic understanding of what each field represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46598ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.get_table_schema(\n",
    "        table_name STRING COMMENT 'Fully qualified table name (catalog.schema.table)'\n",
    "    )\n",
    "    RETURNS TABLE\n",
    "    COMMENT 'Returns detailed schema information via REST API including column names, types, and descriptions'\n",
    "    LANGUAGE SQL\n",
    "    RETURN \n",
    "        SELECT \n",
    "            col.name as column_name,\n",
    "            col.type_text as data_type,\n",
    "            col.comment as column_description,\n",
    "            col.nullable as is_nullable,\n",
    "            col.position as ordinal_position\n",
    "        FROM (\n",
    "            SELECT \n",
    "                CASE \n",
    "                    WHEN table_name NOT IN ({table_list_sql}) THEN\n",
    "                        NULL\n",
    "                    WHEN response.status_code != 200 THEN\n",
    "                        NULL\n",
    "                    ELSE\n",
    "                        from_json(\n",
    "                            response.text,\n",
    "                            'columns ARRAY<STRUCT<name:STRING, type_text:STRING, type_name:STRING, comment:STRING, nullable:BOOLEAN, position:INT>>'\n",
    "                        ).columns\n",
    "                END as columns_array\n",
    "            FROM (\n",
    "                SELECT http_request(\n",
    "                    conn => '{UC_CONNECTION}',\n",
    "                    method => 'GET',\n",
    "                    path => CONCAT('unity-catalog/tables/', table_name),\n",
    "                    headers => map('Accept', 'application/json')\n",
    "                ) as response\n",
    "            )\n",
    "        )\n",
    "        LATERAL VIEW explode(columns_array) exploded_table AS col\n",
    "        WHERE col.name IS NOT NULL\n",
    "        ORDER BY col.position\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67613656",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\n",
    "    f\"SELECT * FROM {CATALOG}.{SCHEMA}.get_table_schema('{UC_TABLES[0]}')\"\n",
    ")\n",
    "\n",
    "display(result.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khzfbw73bh",
   "metadata": {},
   "source": [
    "### Validate Query Function\n",
    "* Purpose: Validates SQL syntax and ensures read-only operations\n",
    "* Why Essential: Prevents write operations and SQL injection attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8208301",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.validate_query(query STRING)\n",
    "    RETURNS STRUCT<is_valid: BOOLEAN, error_message: STRING>\n",
    "    LANGUAGE PYTHON\n",
    "    AS $$\n",
    "    import re\n",
    "\n",
    "    def validate_query(query):\n",
    "        if not query or not query.strip():\n",
    "            return (False, \"Query is empty\")\n",
    "        \n",
    "        query_upper = query.upper()\n",
    "        \n",
    "        # Block write operations\n",
    "        write_keywords = [\n",
    "            'INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER',\n",
    "            'TRUNCATE', 'MERGE', 'REPLACE', 'GRANT', 'REVOKE'\n",
    "        ]\n",
    "        \n",
    "        for kw in write_keywords:\n",
    "            if re.search(r'\\b' + kw + r'\\b', query_upper):\n",
    "                return (False, \"Operation not allowed: \" + kw)\n",
    "        \n",
    "        # Ensure it's a SELECT or CTE (WITH)\n",
    "        stripped = query_upper.strip()\n",
    "        if not (stripped.startswith('SELECT') or stripped.startswith('WITH')):\n",
    "            return (False, \"Only SELECT queries and CTEs are allowed\")\n",
    "        \n",
    "        # Check for SQL injection patterns\n",
    "        injection_patterns = [\n",
    "            r';.*?DROP',\n",
    "            r';.*?DELETE',\n",
    "            r';.*?INSERT',\n",
    "            r'--.*?;',\n",
    "        ]\n",
    "        \n",
    "        for pattern in injection_patterns:\n",
    "            if re.search(pattern, query_upper, re.DOTALL):\n",
    "                return (False, \"Potential SQL injection detected\")\n",
    "        \n",
    "        # Validate balanced parentheses\n",
    "        if query.count('(') != query.count(')'):\n",
    "            return (False, \"Unbalanced parentheses\")\n",
    "        \n",
    "        return (True, None)\n",
    "\n",
    "    return validate_query(query)\n",
    "    $$\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e06511",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT {CATALOG}.{SCHEMA}.validate_query('select * from balance_sheet')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT {CATALOG}.{SCHEMA}.validate_query('drop function some_function')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o3r9kdywsy",
   "metadata": {},
   "source": [
    "### Execute Query Function (Python)\n",
    "* Purpose: Executes SQL query with safety validation and row limits\n",
    "* Why Essential: Safe query execution with automatic limits and error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.execute_query(\n",
    "        sql_query STRING COMMENT 'SQL query to execute (SELECT only)',\n",
    "        row_limit INT DEFAULT 1000 COMMENT 'Maximum number of rows to return (default 1000, max 10000)'\n",
    "    )\n",
    "    RETURNS STRING\n",
    "    COMMENT 'Executes validated SQL query via Statement Execution API with safety checks and row limits. Returns JSON string with results or error message.'\n",
    "    LANGUAGE SQL\n",
    "    RETURN (\n",
    "        SELECT \n",
    "            CASE \n",
    "                -- Step 1: Validate the query first\n",
    "                WHEN {CATALOG}.{SCHEMA}.validate_query(sql_query).is_valid = false THEN\n",
    "                    CONCAT('{{\"error\": \"Validation failed: ', {CATALOG}.{SCHEMA}.validate_query(sql_query).error_message, '\"}}')\n",
    "                \n",
    "                -- Step 2: If valid, execute via API\n",
    "                ELSE\n",
    "                    (\n",
    "                        SELECT \n",
    "                            CASE \n",
    "                                WHEN response.status_code BETWEEN 200 AND 299 THEN\n",
    "                                    response.text\n",
    "                                ELSE\n",
    "                                    CONCAT(\n",
    "                                        '{{\"error\": \"Query execution failed\", \"status_code\": ',\n",
    "                                        CAST(response.status_code AS STRING),\n",
    "                                        ', \"details\": ',\n",
    "                                        COALESCE(response.text, '\"\"'),\n",
    "                                        '}}'\n",
    "                                    )\n",
    "                            END\n",
    "                        FROM (\n",
    "                            SELECT http_request(\n",
    "                                conn => '{UC_CONNECTION_SQL_EXEC}',\n",
    "                                method => 'POST',\n",
    "                                path => 'sql/statements/',\n",
    "                                headers => map('Accept', 'application/json', 'Content-Type', 'application/json'),\n",
    "                                json => CONCAT(\n",
    "                                    '{{\"warehouse_id\": \"{SQL_WAREHOUSE_ID}\", ',\n",
    "                                    '\"statement\": \"', \n",
    "                                    -- Add LIMIT if not present\n",
    "                                    CASE \n",
    "                                        WHEN UPPER(sql_query) LIKE '%LIMIT%' THEN \n",
    "                                            REPLACE(REPLACE(sql_query, '\"', '\\\\\"'), '\\n', ' ')\n",
    "                                        ELSE \n",
    "                                            CONCAT(\n",
    "                                                REPLACE(REPLACE(sql_query, '\"', '\\\\\"'), '\\n', ' '),\n",
    "                                                ' LIMIT ',\n",
    "                                                CAST(LEAST(GREATEST(COALESCE(row_limit, 1000), 1), 10000) AS STRING)\n",
    "                                            )\n",
    "                                    END,\n",
    "                                    '\", \"wait_timeout\": \"30s\", \"on_wait_timeout\": \"CANCEL\", \"format\": \"JSON_ARRAY\"}}'\n",
    "                                )\n",
    "                            ) as response\n",
    "                        )\n",
    "                    )\n",
    "            END\n",
    "    )\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"SELECT {CATALOG}.{SCHEMA}.execute_query('select * from {UC_TABLES[0]} limit 1')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065aawf2o3l",
   "metadata": {},
   "source": [
    "### Get Sample Data Function\n",
    "* Purpose: Returns sample rows from a table to help understand data patterns\n",
    "* Why Essential: LLMs need concrete examples to understand data formats, value patterns, and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.get_sample_data(\n",
    "        table_name STRING COMMENT 'Fully qualified table name (catalog.schema.table)',\n",
    "        num_rows INT DEFAULT 3 COMMENT 'Number of sample rows to return (default 3, max 10)'\n",
    "    )\n",
    "    RETURNS STRING\n",
    "    COMMENT 'Returns sample rows from a table to help understand data patterns and formats. Validates table allowlist and enforces row limits.'\n",
    "    LANGUAGE SQL\n",
    "    RETURN (\n",
    "        SELECT \n",
    "            CASE \n",
    "                -- Validate table is in allowlist\n",
    "                WHEN table_name NOT IN ({table_list_sql}) THEN\n",
    "                    CONCAT('{{\"error\": \"Table not in allowlist: ', table_name, '\"}}')\n",
    "                \n",
    "                -- Validate num_rows is within bounds\n",
    "                WHEN num_rows < 1 OR num_rows > 10 THEN\n",
    "                    '{{\"error\": \"num_rows must be between 1 and 10\"}}'\n",
    "                \n",
    "                -- Execute query via execute_query function\n",
    "                ELSE\n",
    "                    {CATALOG}.{SCHEMA}.execute_query(\n",
    "                        CONCAT('SELECT * FROM ', table_name),\n",
    "                        LEAST(GREATEST(COALESCE(num_rows, 3), 1), 10)\n",
    "                    )\n",
    "            END\n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03121e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT {CATALOG}.{SCHEMA}.get_sample_data('{UC_TABLES[0]}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d667fa",
   "metadata": {},
   "source": [
    "## Future TODOs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1kpg5yn2b7c",
   "metadata": {},
   "source": [
    "### Get Table Relationships Function\n",
    "* Purpose: Returns foreign key relationships for a table\n",
    "* Why Essential: Required for multi-table queries - agent learns correct JOIN syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zpjt48ddny",
   "metadata": {},
   "source": [
    "### Other Advanced Functions\n",
    "\n",
    "* Business Glossary Supporting Table\n",
    "    * Purpose: Maps business terminology to technical column names\n",
    "    * Why Advanced: Critical for enterprise deployments where business users don't know technical schema\n",
    "* Get Business Terms Function\n",
    "    * Purpose: Returns business terminology mappings for domain-specific queries\n",
    "    * Why Advanced: Enables non-technical users to query using familiar business language\n",
    "* Query History Table\n",
    "    * Purpose: Stores successful query patterns for few-shot learning\n",
    "    * Why Advanced: Enables continuous improvement from successful patterns\n",
    "    * WE WILL DO THIS VIA **INFERENCE TABLE**\n",
    "* Get Similar Queries Function\n",
    "    * Purpose: Returns similar successful queries for few-shot learning\n",
    "    * Why Advanced: Improves accuracy by learning from past successful patterns\n",
    "    * WE WILL DO THIS VIA **VECTOR SEARCH INDEX**\n",
    "* Format Query Results Function (Python)\n",
    "    * Purpose: Executes query and formats results in requested format\n",
    "    * Why Advanced: Flexible output formatting for different consumption patterns\n",
    "* Log Query Execution Function\n",
    "    * Purpose: Logs query execution for audit and learning\n",
    "    * Why Advanced: Enables continuous improvement and compliance\n",
    "    * WE WILL DO THIS VIA **INFERENCE TABLE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
