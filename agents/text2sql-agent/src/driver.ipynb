{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text-to-SQL Agent Driver Notebook\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load and test the Text-to-SQL agent\n",
        "2. Log the agent as an MLflow model\n",
        "3. Evaluate the agent with MLflow\n",
        "4. Register and deploy the agent to Unity Catalog\n",
        "\n",
        "For more information, check out [MLflow Agent Framework documentation](https://docs.databricks.com/aws/en/generative-ai/agent-framework/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "This notebook is designed to run in a Databricks notebook environment with access to Unity Catalog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import mlflow\n",
        "\n",
        "# Load configuration\n",
        "configs = mlflow.models.ModelConfig(development_config=\"./config.yml\")\n",
        "databricks_config = configs.get(\"databricks\")\n",
        "agent_config = configs.get(\"agent\")\n",
        "tools_config = configs.get(\"tools\")\n",
        "\n",
        "CATALOG = databricks_config[\"catalog\"]\n",
        "SCHEMA = databricks_config[\"schema\"]\n",
        "UC_MODEL = databricks_config[\"model\"]\n",
        "WORKSPACE_URL = databricks_config[\"workspace_url\"]\n",
        "SQL_WAREHOUSE_ID = databricks_config[\"sql_warehouse_id\"]\n",
        "MLFLOW_EXPERIMENT_ID = databricks_config[\"mlflow_experiment_id\"]\n",
        "\n",
        "UC_TABLES = tools_config[\"tables\"]\n",
        "UC_FUNCTIONS = tools_config.get(\"uc_functions\", [])\n",
        "UC_FUNCTIONS_SCHEMA = f\"{CATALOG}.{SCHEMA}\"\n",
        "UC_CONNECTION = tools_config[\"uc_connection\"][\"name\"]\n",
        "LLM_ENDPOINT = agent_config[\"llm\"][\"endpoint\"]\n",
        "\n",
        "SECRET_SCOPE_NAME = databricks_config.get(\"databricks_pat\").get(\"secret_scope_name\")\n",
        "SECRET_KEY_NAME = databricks_config.get(\"databricks_pat\").get(\"secret_key_name\")\n",
        "\n",
        "os.environ[\"DB_MODEL_SERVING_HOST_URL\"] = WORKSPACE_URL\n",
        "os.environ[\"DATABRICKS_TOKEN\"] = dbutils.secrets.get(\n",
        "    scope=SECRET_SCOPE_NAME, key=SECRET_KEY_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "mlflow.set_registry_uri(\"databricks-uc\")\n",
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "\n",
        "try:\n",
        "    experiment = mlflow.get_experiment(experiment_id=MLFLOW_EXPERIMENT_ID)\n",
        "    mlflow.set_experiment(experiment_id=MLFLOW_EXPERIMENT_ID)\n",
        "    print(f\"Set to existing experiment: {MLFLOW_EXPERIMENT_ID}\")\n",
        "except mlflow.exceptions.RestException as e:\n",
        "    if \"does not exist\" in str(e):\n",
        "        print(f\"Experiment not found. Must create one first.\")\n",
        "    else:\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load & Test Agent\n",
        "\n",
        "Make sure you go to the MLflow experiment to look at trace data as you develop & debug the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from agent import AGENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample questions for testing the text-to-sql agent\n",
        "sample_questions = [\n",
        "    \"What tables are available?\",\n",
        "    \"Show me the schema for the balance_sheet table.\",\n",
        "    \"What were the annual net income over the last 10 years for AAPL?\",\n",
        "    \"Compare annual total assets over the last 10 years between Apple and Bank of America.\",\n",
        "]\n",
        "\n",
        "input_example = {\n",
        "    \"input\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": sample_questions[0],\n",
        "        }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test predict (non-streaming)\n",
        "result = AGENT.predict(input_example)\n",
        "print(result.model_dump(exclude_none=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test predict_stream (streaming)\n",
        "for event in AGENT.predict_stream(input_example):\n",
        "    print(event, \"-----------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log the Agent as an MLflow Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlflow.models.resources import (\n",
        "    DatabricksFunction,\n",
        "    DatabricksSQLWarehouse,\n",
        "    DatabricksServingEndpoint,\n",
        "    DatabricksTable,\n",
        "    DatabricksUCConnection,\n",
        ")\n",
        "\n",
        "# Define resources that the agent depends on\n",
        "resources = [\n",
        "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT),\n",
        "    DatabricksSQLWarehouse(warehouse_id=SQL_WAREHOUSE_ID),\n",
        "    DatabricksUCConnection(connection_name=UC_CONNECTION),\n",
        "]\n",
        "\n",
        "# Add UC Functions as resources from config\n",
        "uc_functions = tools_config.get(\"uc_functions\", [])\n",
        "\n",
        "for function_name in uc_functions:\n",
        "    resources.append(\n",
        "        DatabricksFunction(function_name=f\"{UC_FUNCTIONS_SCHEMA}.{function_name}\")\n",
        "    )\n",
        "\n",
        "# Add tables as resources\n",
        "for table_name in UC_TABLES:\n",
        "    resources.append(DatabricksTable(table_name=table_name))\n",
        "\n",
        "print(\"Resources:\", resources)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mlflow.start_run():\n",
        "    logged_agent_info = mlflow.pyfunc.log_model(\n",
        "        name=\"agent\",\n",
        "        python_model=os.path.join(os.getcwd(), \"agent.py\"),\n",
        "        model_config=os.path.join(os.getcwd(), \"config.yml\"),\n",
        "        code_paths=[os.path.join(os.getcwd(), \"system_prompt.md\")],\n",
        "        input_example=input_example,\n",
        "        resources=resources,\n",
        "        pip_requirements=[\"-r ../requirements.txt\"],\n",
        "    )\n",
        "\n",
        "print(f\"Logged agent: {logged_agent_info.model_uri}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Agent with MLflow\n",
        "\n",
        "Create evaluation questions and use MLflow's GenAI scorers to evaluate agent performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load evaluation dataset if it exists\n",
        "evals_json_path = \"./evals/eval-questions.json\"\n",
        "\n",
        "with open(evals_json_path, \"r\") as f:\n",
        "    eval_dataset_list = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from mlflow.genai.scorers import (\n",
        "    Correctness,\n",
        "    RelevanceToQuery,\n",
        "    Safety,\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "eval_results = mlflow.genai.evaluate(\n",
        "    data=eval_dataset_list,\n",
        "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
        "    scorers=[\n",
        "        Correctness(),\n",
        "        RelevanceToQuery(),\n",
        "        Safety(),\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Evaluation complete. Check MLflow UI for detailed results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Pre-Deployment Agent Validation\n",
        "\n",
        "Test the logged model before deploying to ensure it works correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.models.predict(\n",
        "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
        "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"What tables are available?\"}]},\n",
        "    env_manager=\"uv\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register the Model to Unity Catalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FULL_UC_MODEL_NAME = f\"{CATALOG}.{SCHEMA}.{UC_MODEL}\"\n",
        "\n",
        "uc_registered_model_info = mlflow.register_model(\n",
        "    model_uri=logged_agent_info.model_uri,\n",
        "    name=FULL_UC_MODEL_NAME,\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Registered model: {FULL_UC_MODEL_NAME} (version {uc_registered_model_info.version})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the Agent\n",
        "\n",
        "Deploy the agent to a Model Serving endpoint for production use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks import agents\n",
        "\n",
        "agents.deploy(\n",
        "    FULL_UC_MODEL_NAME,\n",
        "    uc_registered_model_info.version,\n",
        "    tags={\"endpointSource\": \"docs\"},\n",
        "    environment_vars={\n",
        "        \"DATABRICKS_TOKEN\": f\"{{{{secrets/{SECRET_SCOPE_NAME}/{SECRET_KEY_NAME}}}}}\"\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"Deployed {FULL_UC_MODEL_NAME} version {uc_registered_model_info.version}\")\n",
        "print(f\"Access the endpoint at: {WORKSPACE_URL}/ml/endpoints/{FULL_UC_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "* Test the agent endpoint via Playground or the Review App\n",
        "* Continue to iterate on the agent based on evaluation results\n",
        "* Create more comprehensive evaluation datasets\n",
        "* Monitor agent performance using Inference Tables\n",
        "* Set up alerts for query failures or performance issues"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
