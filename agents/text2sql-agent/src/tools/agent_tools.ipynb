{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc5d084",
   "metadata": {},
   "source": [
    "# Create Agent Tools as UC Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b354e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "spark = DatabricksSession.builder.remote(serverless=True).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2cdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "# TODO make sure you update the config file before this\n",
    "\n",
    "configs = mlflow.models.ModelConfig(development_config=\"./config.yml\")\n",
    "databricks_config = configs.get(\"databricks\")\n",
    "tools_config = configs.get(\"tools\")\n",
    "\n",
    "CATALOG = databricks_config[\"catalog\"]\n",
    "SCHEMA = databricks_config[\"schema\"]\n",
    "UC_TABLES = tools_config[\"tables\"]\n",
    "\n",
    "# Format the table list for SQL IN clause\n",
    "table_list_sql = \", \".join([f\"'{table}'\" for table in UC_TABLES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac99a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d03ee9",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ade16",
   "metadata": {},
   "source": [
    "### 1. List Available Tables Function\n",
    "\n",
    "* Purpose: Returns all tables the agent is allowed to query with descriptions\n",
    "* Why Essential: Enables dynamic table discovery without hardcoding table lists in prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.list_available_tables()\n",
    "    RETURNS TABLE\n",
    "    COMMENT 'Returns all tables the agent is allowed to query with descriptions'\n",
    "    LANGUAGE PYTHON\n",
    "    AS $$\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.types import StructType, StructField, StringType\n",
    "        \n",
    "        spark = SparkSession.getActiveSession()\n",
    "        \n",
    "        # List of allowed tables from config\n",
    "        allowed_tables = {UC_TABLES}\n",
    "        \n",
    "        results = []\n",
    "        for table in allowed_tables:\n",
    "            try:\n",
    "                # Use DESCRIBE EXTENDED which is much faster than information_schema\n",
    "                desc = spark.sql(f\"DESCRIBE EXTENDED {{table}}\")\n",
    "                \n",
    "                # Extract table comment from DESCRIBE EXTENDED output\n",
    "                table_comment = None\n",
    "                for row in desc.collect():\n",
    "                    if row['col_name'] and 'Comment' in row['col_name']:\n",
    "                        table_comment = row['data_type']\n",
    "                        break\n",
    "                \n",
    "                # Get table type using DESCRIBE DETAIL (also fast)\n",
    "                detail = spark.sql(f\"DESCRIBE DETAIL {{table}}\").collect()[0]\n",
    "                table_type = detail['format'] if 'format' in detail.asDict() else 'TABLE'\n",
    "                \n",
    "                results.append({{\n",
    "                    'full_table_name': table,\n",
    "                    'table_name': table.split('.')[-1],\n",
    "                    'description': table_comment,\n",
    "                    'table_type': table_type\n",
    "                }})\n",
    "            except Exception as e:\n",
    "                # Skip tables that don't exist or can't be accessed\n",
    "                continue\n",
    "        \n",
    "        # Return as DataFrame\n",
    "        schema = StructType([\n",
    "            StructField('full_table_name', StringType(), True),\n",
    "            StructField('table_name', StringType(), True),\n",
    "            StructField('description', StringType(), True),\n",
    "            StructField('table_type', StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        return spark.createDataFrame(results, schema)\n",
    "    $$\n",
    "    ;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0899ae",
   "metadata": {},
   "source": [
    "### 2. Get Table Schema Function\n",
    "* Purpose: Returns detailed schema information for a specific table\n",
    "* Why Essential: Provides column names, types, and descriptions for SQL generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1097909",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.get_table_schema(\n",
    "        table_name STRING COMMENT 'Fully qualified table name (catalog.schema.table)'\n",
    "    )\n",
    "    RETURNS TABLE\n",
    "    COMMENT 'Returns detailed schema information for a table including column names, types, and descriptions'\n",
    "    LANGUAGE PYTHON\n",
    "    AS $$\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "        \n",
    "        spark = SparkSession.getActiveSession()\n",
    "        \n",
    "        # Security check: only allow querying approved tables\n",
    "        allowed_tables = {UC_TABLES}\n",
    "        if table_name not in allowed_tables:\n",
    "            raise ValueError(f\"Access denied: table {{table_name}} is not in the allowed list\")\n",
    "        \n",
    "        try:\n",
    "            # Use DESCRIBE EXTENDED for fast metadata access\n",
    "            desc_df = spark.sql(f\"DESCRIBE EXTENDED {{table_name}}\")\n",
    "            \n",
    "            results = []\n",
    "            position = 1\n",
    "            \n",
    "            for row in desc_df.collect():\n",
    "                col_name = row['col_name']\n",
    "                data_type = row['data_type']\n",
    "                comment = row['comment']\n",
    "                \n",
    "                # Skip metadata rows (they start with # or are empty)\n",
    "                if not col_name or col_name.startswith('#') or col_name == '':\n",
    "                    break\n",
    "                    \n",
    "                # Determine nullability (default to True if not specified)\n",
    "                is_nullable = 'NOT NULL' not in str(data_type).upper()\n",
    "                \n",
    "                results.append({{\n",
    "                    'column_name': col_name,\n",
    "                    'data_type': data_type,\n",
    "                    'comment': comment if comment else None,\n",
    "                    'is_nullable': 'YES' if is_nullable else 'NO',\n",
    "                    'ordinal_position': position\n",
    "                }})\n",
    "                \n",
    "                position += 1\n",
    "            \n",
    "            # Return as DataFrame\n",
    "            schema = StructType([\n",
    "                StructField('column_name', StringType(), True),\n",
    "                StructField('data_type', StringType(), True),\n",
    "                StructField('comment', StringType(), True),\n",
    "                StructField('is_nullable', StringType(), True),\n",
    "                StructField('ordinal_position', IntegerType(), True)\n",
    "            ])\n",
    "            \n",
    "            return spark.createDataFrame(results, schema)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to get schema for {{table_name}}: {{str(e)}}\")\n",
    "    $$\n",
    "    ;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065aawf2o3l",
   "metadata": {},
   "source": [
    "### 3. Get Sample Data Function\n",
    "* Purpose: Returns sample rows from a table to help understand data patterns\n",
    "* Why Essential: LLMs need concrete examples to understand data formats, value patterns, and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xr1bnt798ko",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.get_sample_data(\n",
    "        table_name STRING COMMENT 'Fully qualified table name',\n",
    "        num_rows INT DEFAULT 3 COMMENT 'Number of sample rows (default: 3, max: 10)'\n",
    "    )\n",
    "    RETURNS TABLE\n",
    "    COMMENT 'Returns sample rows from a table to help understand data patterns'\n",
    "    LANGUAGE SQL\n",
    "    RETURN\n",
    "        SELECT *\n",
    "        FROM IDENTIFIER(table_name)\n",
    "        LIMIT LEAST(num_rows, 10)\n",
    "    ;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1kpg5yn2b7c",
   "metadata": {},
   "source": [
    "### 4. Get Table Relationships Function\n",
    "* Purpose: Returns foreign key relationships for a table\n",
    "* Why Essential: Required for multi-table queries - agent learns correct JOIN syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yw19uf6u0i",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.get_table_relationships(\n",
    "        table_name STRING COMMENT 'Fully qualified table name'\n",
    "    )\n",
    "    RETURNS TABLE\n",
    "    COMMENT 'Returns foreign key relationships for a table to enable JOIN generation'\n",
    "    LANGUAGE PYTHON\n",
    "    AS $$\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql.types import StructType, StructField, StringType\n",
    "        \n",
    "        spark = SparkSession.getActiveSession()\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            # Try to get constraint information using SHOW TBLPROPERTIES\n",
    "            # This is faster than information_schema\n",
    "            try:\n",
    "                constraints = spark.sql(f\"SHOW TBLPROPERTIES {{table_name}}\")\n",
    "                \n",
    "                # Look for foreign key constraints in table properties\n",
    "                for row in constraints.collect():\n",
    "                    if 'constraint' in str(row['key']).lower() or 'foreign' in str(row['key']).lower():\n",
    "                        # Parse constraint info if available\n",
    "                        # Note: This is implementation-specific and may vary\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # If no constraints found via properties, return empty result\n",
    "            # Note: Unity Catalog constraints can also be queried via:\n",
    "            # spark.sql(f\"DESCRIBE DETAIL {{table_name}}\") and checking properties\n",
    "            \n",
    "            schema = StructType([\n",
    "                StructField('from_table', StringType(), True),\n",
    "                StructField('foreign_key_column', StringType(), True),\n",
    "                StructField('to_table', StringType(), True),\n",
    "                StructField('referenced_column', StringType(), True),\n",
    "                StructField('constraint_name', StringType(), True)\n",
    "            ])\n",
    "            \n",
    "            return spark.createDataFrame(results, schema)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Return empty DataFrame on error\n",
    "            schema = StructType([\n",
    "                StructField('from_table', StringType(), True),\n",
    "                StructField('foreign_key_column', StringType(), True),\n",
    "                StructField('to_table', StringType(), True),\n",
    "                StructField('referenced_column', StringType(), True),\n",
    "                StructField('constraint_name', StringType(), True)\n",
    "            ])\n",
    "            return spark.createDataFrame([], schema)\n",
    "    $$\n",
    "    ;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khzfbw73bh",
   "metadata": {},
   "source": [
    "### 5. Validate Query Function (Python)\n",
    "* Purpose: Validates SQL syntax and ensures read-only operations\n",
    "* Why Essential: Prevents write operations and SQL injection attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0rtrdnopa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.validate_query(\n",
    "        query STRING COMMENT 'SQL query to validate'\n",
    "    )\n",
    "    RETURNS STRUCT<is_valid: BOOLEAN, error_message: STRING>\n",
    "    COMMENT 'Validates SQL syntax and ensures read-only operations (no INSERT/UPDATE/DELETE)'\n",
    "    LANGUAGE PYTHON\n",
    "    AS $$\n",
    "        import re\n",
    "\n",
    "        # Normalize query for checking\n",
    "        query_upper = query.upper()\n",
    "        query_stripped = query.strip()\n",
    "\n",
    "        # Block write operations\n",
    "        write_keywords = [\n",
    "            'INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE',\n",
    "            'ALTER', 'TRUNCATE', 'MERGE', 'GRANT', 'REVOKE',\n",
    "            'REPLACE', 'RENAME', 'COPY', 'LOAD', 'UNLOAD'\n",
    "        ]\n",
    "\n",
    "        for keyword in write_keywords:\n",
    "            # Use word boundary to avoid false positives\n",
    "            if re.search(rf'\\\\b{{keyword}}\\\\b', query_upper):\n",
    "                return {{\n",
    "                    'is_valid': False,\n",
    "                    'error_message': f'Operation {{keyword}} is not allowed. Only SELECT queries are permitted.'\n",
    "                }}\n",
    "\n",
    "        # Ensure query starts with SELECT or WITH (for CTEs)\n",
    "        if not re.match(r'^(SELECT|WITH)\\\\b', query_stripped, re.IGNORECASE):\n",
    "            return {{\n",
    "                'is_valid': False,\n",
    "                'error_message': 'Only SELECT queries (and WITH/CTEs) are allowed.'\n",
    "            }}\n",
    "\n",
    "        # Check for SQL injection patterns\n",
    "        injection_patterns = [\n",
    "            r';.*?DROP',\n",
    "            r';.*?DELETE',\n",
    "            r'--.*?(DROP|DELETE|UPDATE)',\n",
    "            r'/\\\\*.*?(DROP|DELETE|UPDATE).*?\\\\*/'\n",
    "        ]\n",
    "\n",
    "        for pattern in injection_patterns:\n",
    "            if re.search(pattern, query_upper):\n",
    "                return {{\n",
    "                    'is_valid': False,\n",
    "                    'error_message': 'Potentially malicious SQL pattern detected.'\n",
    "                }}\n",
    "\n",
    "        # Basic syntax validation (check for balanced parentheses)\n",
    "        if query.count('(') != query.count(')'):\n",
    "            return {{\n",
    "                'is_valid': False,\n",
    "                'error_message': 'Syntax error: Unbalanced parentheses.'\n",
    "            }}\n",
    "\n",
    "        return {{'is_valid': True, 'error_message': None}}\n",
    "    $$\n",
    "    ;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o3r9kdywsy",
   "metadata": {},
   "source": [
    "### 6. Execute Query Function (Python)\n",
    "* Purpose: Executes SQL query with safety validation and row limits\n",
    "* Why Essential: Safe query execution with automatic limits and error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k1t3zgyeh1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION {CATALOG}.{SCHEMA}.execute_query(\n",
    "        sql_query STRING COMMENT 'SQL query to execute',\n",
    "        row_limit INT DEFAULT 1000 COMMENT 'Maximum rows to return (default: 1000)'\n",
    "    )\n",
    "    RETURNS TABLE\n",
    "    COMMENT 'Executes SQL query with safety validation, row limits, and timeout protection'\n",
    "    LANGUAGE PYTHON\n",
    "    AS $$\n",
    "        import re\n",
    "        from pyspark.sql import SparkSession\n",
    "\n",
    "        spark = SparkSession.getActiveSession()\n",
    "\n",
    "        # Validate query is read-only using our validation function\n",
    "        validation = spark.sql(f\\\"\\\"\\\"\n",
    "            SELECT {CATALOG}.{SCHEMA}.validate_query('{{sql_query.replace(\"'\", \"''\")}}') as result\n",
    "        \\\"\\\"\\\").collect()[0]['result']\n",
    "\n",
    "        if not validation['is_valid']:\n",
    "            raise ValueError(f\"Query validation failed: {{validation['error_message']}}\")\n",
    "\n",
    "        # Add row limit for safety (cap at 10,000)\n",
    "        safe_limit = min(row_limit, 10000)\n",
    "\n",
    "        # Check if query already has LIMIT clause\n",
    "        if re.search(r'\\\\bLIMIT\\\\s+\\\\d+', sql_query, re.IGNORECASE):\n",
    "            safe_query = sql_query\n",
    "        else:\n",
    "            safe_query = f\"SELECT * FROM ({{sql_query}}) LIMIT {{safe_limit}}\"\n",
    "\n",
    "        try:\n",
    "            result = spark.sql(safe_query)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Query execution failed: {{str(e)}}\")\n",
    "    $$\n",
    "    ;\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xkq4k94knik",
   "metadata": {},
   "source": [
    "## Enhanced Functions (High Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3pyily6ch06",
   "metadata": {},
   "source": [
    "### 7. Search Tables by Keyword Function\n",
    "* Purpose: Searches for tables and columns matching keyword\n",
    "* Why High Value: Enables semantic schema discovery - agent can find relevant tables for \"revenue\", \"customers\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zpjt48ddny",
   "metadata": {},
   "source": [
    "## Advanced Functions (Future Enhancements)\n",
    "\n",
    "* Business Glossary Supporting Table\n",
    "    * Purpose: Maps business terminology to technical column names\n",
    "    * Why Advanced: Critical for enterprise deployments where business users don't know technical schema\n",
    "* Get Business Terms Function\n",
    "    * Purpose: Returns business terminology mappings for domain-specific queries\n",
    "    * Why Advanced: Enables non-technical users to query using familiar business language\n",
    "* Query History Table\n",
    "    * Purpose: Stores successful query patterns for few-shot learning\n",
    "    * Why Advanced: Enables continuous improvement from successful patterns\n",
    "    * WE WILL DO THIS VIA **INFERENCE TABLE**\n",
    "* Get Similar Queries Function\n",
    "    * Purpose: Returns similar successful queries for few-shot learning\n",
    "    * Why Advanced: Improves accuracy by learning from past successful patterns\n",
    "    * WE WILL DO THIS VIA **VECTOR SEARCH INDEX**\n",
    "* Format Query Results Function (Python)\n",
    "    * Purpose: Executes query and formats results in requested format\n",
    "    * Why Advanced: Flexible output formatting for different consumption patterns\n",
    "* Log Query Execution Function\n",
    "    * Purpose: Logs query execution for audit and learning\n",
    "    * Why Advanced: Enables continuous improvement and compliance\n",
    "    * WE WILL DO THIS VIA **INFERENCE TABLE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
